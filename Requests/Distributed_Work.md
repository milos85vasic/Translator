Now we have to extend the system to support distributed work! We are going to provide a pool of ssh access es to remote machines. Once connected the following will happen: on remote machine we check if our apps are already available and running - REST service. Then, we are going to "PAIR" with remote instance of our project. Once paired - securely, using http3 / quic / cronet we are going to use the resources of remote host machine - all LLMs cofigured and accessible! The ones with API keys (if any) and, if available and configured / requested - the Ollama or llama.cpp! Rules for locally hosted LLMs still apply to remote paired hosts: Determine how many LLM instances can realistically start and run for the translation purposes so we do not get into same situation. We shall aways run local LLMs according to capabilities of host machine, but it also take into the account how many instances can run at the same time on  such host machine! Document properly use of distributed workers! Make sure that no sensitive data - workers configurations - are git versioned. We could only git version example configurations. Cover everything with the tests - unit, integration, full automation, security, benchmark and stress tests. Each feature must be covered with 100% with each tests type. All test must complete with success. For the testing instantiate test instance worker using dokcer (docker compose). Extend properly the REST API. For everything that happens in the system proper web socket events must be emitted. Root instance of the app will listen for paired workers and receive all of their emitted events, and then re-emmit properly to subscribed listeners of the main app instance!